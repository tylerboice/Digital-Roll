<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Unknown </title></head><body>
<h1 id="training-instruction">Training Instruction</h1>
<h2 id="voc-2012-dataset-from-scratch">VOC 2012 Dataset from Scratch</h2>
<p>Full instruction on how to train using VOC 2012 from scratch</p>
<p>Requirement:
  1. Able to detect image using pretrained darknet model
  2. Many Gigabytes of Disk Space
  3. High Speed Internet Connection Preferred
  4. GPU Preferred</p>
<h3 id="1-download-dataset">1. Download Dataset</h3>
<p>You can read the full description of dataset <a href="http://host.robots.ox.ac.uk/pascal/VOC/">here</a>
<code>bash
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar -O ./data/voc2012_raw.tar
mkdir -p ./data/voc2012_raw
tar -xf ./data/voc2012_raw.tar -C ./data/voc2012_raw
ls ./data/voc2012_raw/VOCdevkit/VOC2012 # Explore the dataset</code></p>
<h3 id="2-transform-dataset">2. Transform Dataset</h3>
<p>See tools/voc2012.py for implementation, this format is based on <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">tensorflow object detection API</a>. Many fields 
are not required, I left them there for compatibility with official API.</p>
<p>```bash
python tools/voc2012.py \
  --data_dir './data/voc2012_raw/VOCdevkit/VOC2012' \
  --split train \
  --output_file ./data/voc2012_train.tfrecord</p>
<p>python tools/voc2012.py \
  --data_dir './data/voc2012_raw/VOCdevkit/VOC2012' \
  --split val \
  --output_file ./data/voc2012_val.tfrecord
```</p>
<p>You can visualize the dataset using this tool
<code>python tools/visualize_dataset.py --classes=./data/voc2012.names</code></p>
<p>It will output one random image with label to <code>output.jpg</code></p>
<h3 id="3-training">3. Training</h3>
<p>You can adjust the parameters based on your setup</p>
<h4 id="with-transfer-learning">With Transfer Learning</h4>
<p>This step requires loading the pretrained darknet (feature extractor) weights.
```
wget https://pjreddie.com/media/files/yolov3.weights -O data/yolov3.weights
python convert.py
python detect.py --image ./data/meme.jpg # Sanity check</p>
<p>python train.py \
    --dataset ./data/voc2012_train.tfrecord \
    --val_dataset ./data/voc2012_val.tfrecord \
    --classes ./data/voc2012.names \
    --num_classes 20 \
    --mode fit --transfer darknet \
    --batch_size 16 \
    --epochs 10 \
    --weights ./checkpoints/yolov3.tf \
    --weights_num_classes 80 
```</p>
<p>Original pretrained yolov3 has 80 classes, here we demonstrated how to
do transfer learning on 20 classes.</p>
<h4 id="training-from-random-weights-not-recommended">Training from random weights (NOT RECOMMENDED)</h4>
<p>Training from scratch is very difficult to converge
The original paper trained darknet 
on imagenet before training the whole network as well.</p>
<p><code>bash
python train.py \
    --dataset ./data/voc2012_train.tfrecord \
    --val_dataset ./data/voc2012_val.tfrecord \
    --classes ./data/voc2012.names \
    --num_classes 20 \
    --mode fit --transfer none \
    --batch_size 16 \
    --epochs 10 \</code></p>
<p>I have tested this works 100% with correct loss and converging over time.
Each epoch takes around 10 minutes on single AWS p2.xlarge (Nvidia K80 GPU) Instance.</p>
<p>You might see warnings or error messages during training, they are not critical dont' worry too much about them.
There might be a long wait time between each epoch becaues we are calculating validation loss.</p>
<h3 id="4-inference">4. Inference</h3>
<p>```bash</p>
<h1 id="detect-from-images">detect from images</h1>
<p>python detect.py \
    --classes ./data/voc2012.names \
    --num_classes 20 \
    --weights ./checkpoints/yolov3_train_5.tf \
    --image ./data/street.jpg</p>
<h1 id="detect-from-validation-set">detect from validation set</h1>
<p>python detect.py \
    --classes ./data/voc2012.names \
    --num_classes 20 \
    --weights ./checkpoints/yolov3_train_5.tf \
    --tfrecord ./data/voc2012_val.tfrecord
```</p>
<p>You should see some detect objects in the standard output and the visualization at <code>output.jpg</code>.
this is just a proof of concept, so it won't be as good as pretrained models.
In my experience, you might need lower score score thershold if you didn't train it enough.</p>
</body></html>